/* eslint-disable camelcase */
import { encode, isWithinTokenLimit } from 'gpt-tokenizer';
import openai, { OpenAI } from 'openai';

export type Role = 'system' | 'user' | 'assistant';

export type OpenAICompletionSession = {
  sendPrompt: (prompt: string) => Promise<SendPromptResponse>;
};

export type Message = {
  role: Role;
  content: string;
};

export type SendPromptResponse = {
  /**
   * Textual response generated by the AI model
   */
  response: string;
  /**
   * Conversation history including the prompt and all responses
   */
  conversation: Message[];
  /**
   * Total number of tokens sent to the API in this session
   */
  sessionInputTokens: number;
  /**
   * Total number of tokens received from the API in this session
   */
  sessionOutputTokens: number;
};

export type CompletionOpenAIParams = {
  model: openai.ChatModel;
  seed?: number | null;
  temperature?: number | null;
  top_p?: number | null;
  frequency_penalty?: number | null;
  presence_penalty?: number | null;
  /**
   * Maximum number of tokens that the API is allowed to generate in their answers
   */
  max_tokens?: number | null;
  stop?: string | null;
};

export type CompletionOptions = {
  openaiConfig: CompletionOpenAIParams;
  /**
   * Maximum number of allowed prompts sent to the model in this session
   * This is a safety measure to prevent infinite loops and overuse of the API in case of bugs
   * Defaults to 5
   */
  maxPrompts?: number;
  /**
   * The maximum number of tokens allowed to be sent to the API in a single request,
   * including the prompt and all previous history in the conversation.
   * Defaults to 4000 tokens
   */
  maxTokensPerRequest?: number;
  /**
   * Maximum number of tokens allowed to be sent/received to the API in this whole session.
   * This is the sum of all request and response tokens sent/received throughout different calls to sendPrompt()
   * Remember that when sending a prompt, the prompt and all previous history in the conversation are re-sent, counting again towards the limit.
   * For example:
   *   - if you have a history of 1000 tokens and send a prompt of 10 tokens with an answer of 5 tokens, the total tokens sent/received will be 1015
   *   - now if you send another prompt of 10 tokens, receiving 5 tokens again, the total tokens sent/received will be 2030 tokens.
   * This is a safety measure to prevent generating too much content that might indicate bugs even before invoking the API leading to high costs
   * Defaults to 12000 tokens
   */
  maxTokensTotal?: number;
};

export const createOpenAICompletionSession = (
  openaiClient: OpenAI,
  completionOptions: CompletionOptions,
): OpenAICompletionSession => {
  const conversation: Message[] = [
    { role: 'system', content: 'You are an AI assistant that helps people find information.' },
  ];
  let promptCounter = 0;

  const maxPrompts = completionOptions.maxPrompts || 5;
  const maxTokensPerRequest = completionOptions.maxTokensPerRequest || 4000;
  const maxTokensTotal = completionOptions.maxTokensTotal || 12000;

  let sessionInputTokens = 0;
  let sessionOutputTokens = 0;

  return {
    sendPrompt: async (prompt: string): Promise<SendPromptResponse> => {
      // check max prompts
      promptCounter += 1;
      if (promptCounter > maxPrompts) {
        throw new Error(`Too many prompts in this session (${promptCounter}/${maxPrompts})`);
      }
      conversation.push({ role: 'user', content: prompt });

      // check max tokens in this session
      // '_' is added because if input is empty, isWithinTokenLimit function returns false
      const fullContents = `_ ${JSON.stringify(conversation.map((m) => m.content))}`;
      const requestTokensCount = encode(fullContents).length;

      // the actual totalTokensCounter will be updated from the response usage info later on
      // but for now we do a check based on the request tokens to avoid sending too many tokens to the API beforehand
      const sessionTotalTokens = sessionInputTokens + sessionOutputTokens + requestTokensCount;
      if (sessionTotalTokens > maxTokensTotal) {
        throw new Error(
          `Exceeded max total tokens sent/received to/from the API in this session. ${sessionTotalTokens}/${maxTokensTotal}`,
        );
      }

      if (!isWithinTokenLimit(fullContents, maxTokensPerRequest)) {
        throw new Error(
          `Exceeded max tokens per request. ${requestTokensCount}/${maxTokensPerRequest}`,
        );
      }

      // send request to openai api
      const response = await openaiClient.chat.completions.create({
        ...completionOptions.openaiConfig,
        messages: conversation,
        stream: false,
      });

      // process response
      const completion = response.choices[0].message.content;

      if (!response.usage?.total_tokens) {
        throw new Error('response.usage.total_tokens is empty');
      }
      sessionOutputTokens += response.usage.completion_tokens;
      sessionInputTokens += response.usage.prompt_tokens;

      if (!completion) {
        throw new Error('Response message content is empty');
      }
      conversation.push({ role: 'assistant', content: completion });

      return {
        response: completion,
        conversation,
        sessionInputTokens,
        sessionOutputTokens,
      };
    },
  };
};
