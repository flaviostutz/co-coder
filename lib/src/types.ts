import openai, { OpenAI } from 'openai';

import { ProgressLogFunc, ProgressLogLevel } from './progressLog';

export type Role = 'system' | 'user' | 'assistant';

export type SessionStats = {
  promptCounter: number;
  sessionInputTokens: number;
  sessionOutputTokens: number;
};

export type Message = {
  role: Role;
  content: string;
};

export type OpenAICompletionSession = {
  sendPrompt: (prompt: string) => Promise<SendPromptResponse>;
  stats: () => SessionStats;
};

export type SendPromptResponse = {
  /**
   * Textual response generated by the AI model
   */
  response: string;
  /**
   * Conversation history including the prompt and all responses
   */
  conversation: Message[];
  /**
   * Total number of tokens sent to the API in this session
   */
  sessionInputTokens: number;
  /**
   * Total number of tokens received from the API in this session
   */
  sessionOutputTokens: number;
};

export type CompletionOpenAIParams = {
  model: openai.ChatModel;
  seed?: number | null;
  temperature?: number | null;
  top_p?: number | null;
  frequency_penalty?: number | null;
  presence_penalty?: number | null;
  /**
   * Maximum number of tokens that the API is allowed to generate in their answers
   */
  max_tokens?: number | null;
  stop?: string | null;
};

export type CompletionOptions = {
  openaiConfig: CompletionOpenAIParams;
  /**
   * Maximum number of allowed prompts sent to the model in this session
   * This is a safety measure to prevent infinite loops and overuse of the API in case of bugs
   * Defaults to 5
   */
  maxPrompts?: number;
  /**
   * The maximum number of tokens allowed to be sent to the API in a single request,
   * including the prompt and all previous history in the conversation.
   * Defaults to 4000 tokens
   */
  maxTokensPerRequest?: number;
  /**
   * Maximum number of tokens allowed to be sent/received to the API in this whole session.
   * This is the sum of all request and response tokens sent/received throughout different calls to sendPrompt()
   * Remember that when sending a prompt, the prompt and all previous history in the conversation are re-sent, counting again towards the limit.
   * For example:
   *   - if you have a history of 1000 tokens and send a prompt of 10 tokens with an answer of 5 tokens, the total tokens sent/received will be 1015
   *   - now if you send another prompt of 10 tokens, receiving 5 tokens again, the total tokens sent/received will be 2030 tokens.
   * This is a safety measure to prevent generating too much content that might indicate bugs even before invoking the API leading to high costs
   * Defaults to 12000 tokens
   */
  maxTokensTotal?: number;
};

export type PromptProcessResult = {
  /**
   * List of file paths generated by the prompt
   */
  generatedFiles: string[];
  notes: string[];
  stats: SessionStats;
};

/**
 * Arguments for sending a prompt to ChatGPT and processing the response
 */
export type SendAndProcessPromptArgs = {
  /**
   * Prompt to be sent to OpenAI API
   */
  prompt: string;
  /**
   * Directory where the workspace files will be read when
   * additional files are requested by the model
   */
  requestedFilesDir: string;
  /**
   * Output directory where the generated files will be written
   * @required
   */
  outputDir: string;
  /**
   * OpenAI client to be used for generating code
   */
  openAIClient: OpenAI; // Replace 'any' with the actual type if known
  /**
   * OpenAI model to be used for generating code
   */
  model: openai.ChatModel;
  /**
   * Maximum number of tokens allowed to be sent to the API in a single request
   * Defaults to 4000
   */
  maxTokensPerRequest?: number;
  /**
   * Maximum number of tokens allowed to be sent to the API in total, counting different requests
   * Defaults to 12000
   */
  maxTokensTotal?: number;
  /**
   * OpenAI completion session to be used for generating code
   * Used to track the conversation state.
   * Normally left undefined as it's supposed to be used to continue a conversation from a previous session.
   * If defined, "model" param is ignored.
   * If not provided, a new session will be created using the "model" parameter.
   */
  openAICompletionSession?: OpenAICompletionSession;
  /**
   * Define this function to receive logs as the prompt is being processed
   */
  progressLogFunc?: ProgressLogFunc;
  /**
   * Define the log level for the progress log
   * @default 'info'
   */
  progressLogLevel?: ProgressLogLevel;
  requestedFilesLimits: FileContentsLimits;
};

/**
 * Arguments for running the workspace prompt runner
 */
export type WorkspacePromptRunnerArgs = Pick<
  SendAndProcessPromptArgs,
  | 'outputDir'
  | 'openAIClient'
  | 'model'
  | 'maxTokensPerRequest'
  | 'maxTokensTotal'
  | 'progressLogFunc'
  | 'progressLogLevel'
  | 'requestedFilesLimits'
> & {
  /**
   * Arguments for generating the code prompt
   */
  codePromptGeneratorArgs: CodePromptGeneratorArgs;
};

export type FileContentsLimits = {
  /**
   * Regexes of filenames to ignore so it's contents won't be fetched
   * even if the filename matches filenameRegexes
   */
  ignoreFilenameRegexes?: string[];
  /**
   * Maximum individual file size to be included in the prompt. Larger files will be truncated
   * @default 20000
   */
  maxFileSize?: number;
  /**
   * Max number of tokens with the contents of the files.
   * Stops processing files if the token limit is reached. This will simply ignore the rest of the files
   * and return everything that was processed until that point.
   * This is a safety measure to prevent sending too much data to the API
   * @default 50000
   */
  maxTokens?: number;
};

export type PromptFileContentsArgs = FileContentsLimits & {
  /**
   * Base directory where the workspace files are located
   * @required
   */
  baseDir: string;
  /**
   * Regexes to filter files and get contents.
   * It will be used to get the contents of the files to be sent to the OpenAI API.
   * @required
   */
  filenameRegexes: string[];
};

export type PromptFileContentsResponse = {
  /**
   * Filename and contents of all files that matched the regexes in prompt format
   */
  fileContentsPrompt: string;
  /**
   * Total files that were added to the response as a prompt
   * These include truncated and complete files
   */
  filesProcessed: string[];
  /**
   * Total files that were skipped because the total token limit was reached
   */
  filesSkipped: string[];
  /**
   * Total files that were truncated because they were too large (related to maxSize)
   * These files were added to the response, but their contents were truncated
   */
  filesTruncated: string[];
};

/**
 * Arguments for generating a code prompt
 */
export type CodePromptGeneratorArgs = {
  /**
   * Description of the task to be performed. It will be added to the prompt as the main task that must be performed by the model.
   * A bunch of other instructions will be added besides this task to the prompt that is sent to OpenAI API
   * @required
   */
  taskDescription: string;
  /**
   * Information about the project that the task is related to. It will be added to the prompt as the context of the task.
   * Add here informations such as the project's purpose, the technologies used, the structure of the project, etc.
   * You can use markdown format to make the content more readable.
   */
  projectInformation?: string;
  /**
   * Workspace files that will be sent along with the prompt to the OpenAI API so it can use it as a context to generate the codes.
   * It will use these files to understand the structure of the project, technologies used and other informations that can help to generate the code.
   * @required
   */
  workspaceFiles: {
    fullContents?: PromptFileContentsArgs;
    previewContents?: PromptFileContentsArgs;
  };
  /**
   * Example of the task to be performed. It will be added to the prompt as an example of the task that must be performed by the model.
   * You fully describe an example for the task, or indicate which files or folders can be used as an example and it will try to generate a code based on this example.
   * e.g.: "Use workspace files under folder `packages/reference-runner` as an example"
   */
  example: string;
};

export type CodePromptGeneratorResponse = {
  codePrompt: string;
  fullFileContents?: PromptFileContentsResponse;
  previewFileContents?: PromptFileContentsResponse;
};
